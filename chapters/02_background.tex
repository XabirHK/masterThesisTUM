% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Background}\label{chapter:background}

\section{Sensor}

Our modern world us infused with sensors. We use sensors in everyday life concisely or subconsciously. From our smartphone, cars to the state of the art medical equipment, mars roves has a variety of uses of sensors. Although when the term sensor comes up we think about mostly mechanical sensors, such as touchscreen in our smartphone or the fire detection sensor, sensors can be not only mechanical but also chemical, bio, and other variety of types.

These sensors are collecting data every moment in our modern world. For example, our buses, trains, planes, and ships are using GPS to navigate around the world. We are using our smartphone which has a variety of sensors working together like the camera, touch sensors, gyro, GPS, and proximity sensor. They are working continuously in the background and processing data to make our life a  little bit easier. Now we can sense and track out heartbeat just using a wristwatch.

The use of smart sensors in our homes, offices, shopping malls, cars is getting bigger and bigger nowadays. All modern means of transport such as cars, buses, trains are using sensors to create a more comfortable transport experience. For instance, proximity sensors for better parking experience, crash prevention. If we talk about air transportation, the impact of sensors is even more important and critical for instance temperature, air pressure sensors. A single failure of any of these sensors could lead to deadly outcomes with fatality. The list goes on when it comes to the use of sensors and their impacts on our life

n the broadest definition, a sensor is a device that can  monitor the environment that its designed for. A sensor is a device or an integrated system of devices that senses and responds to certain types of environmental inputs. Light, heat, movement, moisture, pressure, or any of a number of other environmental phenomena could be the specific input. The output is usually a signal converted for reading or continuing processing to a human-readable display at the point of the sensor or electronically transmitted over a network. Due to the different needs, the properties of sensors were changed a lot over time. The need enabled intelligent and intelligent sensors to be created. The advent of intelligent sensors led to the development of microcontrollers used everywhere. The smart sensors have allowed us to interface with other devices and make them work together as one device.


\subsection{Sensor Types}

As we mentioned before, sensors can be many types, It can be categorized depending on their purpose or depending on how they are made. There are various sensor classifications created by different writers and specialists. An expert in the subject can already use the following sensor classification, but it is an extremely simple sensor classification. They are divided into active and passive in the first classification of the sensors. Active sensors are those that require a power signal or an external excitation signal. On the other hand, passive sensors do not require an external power signal and generate an output response directly. Analog and optical sensors are the final grouping of the sensors. Analog capabilities generate an analog output.

We have a long list of sensors, which we use in our everyday work. If we want to speak briefly about them, we can hardly cover each sensor. We will discuss a few of them which is somewhat related to our work. The sensor mention below are regardless of their classes, they are a mix of digital, analog, active sensors.

\subsubsection{Temperature sensors or thermal sensors}

This system collects temperature information from a system and transforms it into a way that another system or individual can understand. Mercury in a glass thermometer is the simplest example of a temperature sensor. Mercury expands in the glass and contracts to rely on temperature changes. The external temperature is the source factor for measuring the temperature. In order to calculate the temperature, the spectator measures the location of mercury. Two basic temperature sensors are available:

\begin{figure}[h]
  \centering
    \includegraphics[width=0.8\linewidth]{figures/tempSensor.jpg}
    \caption{A typical temperature sensor.}
    \source {emerson.com}
\label{fig:tempSensor}
\end{figure}


\begin{itemize}
 \item Contact sensors – This type of sensor requires physical direct contact with the sensed object or media. They monitor solids, liquids, and gas temperatures across a broad range of temperatures.
 \item  Non-contact Sensors – No physical contact with the object or the media is required for this type of sensor. They monitor non-reflective solids and liquids, but because of natural transparency, they are not useful for gasses. These sensors use the Law of Plank for temperature calculation. This law covers heat radiated from the heat source for temperature measurements.

\end{itemize}


\subsubsection{Sonar sensors}


Sonar sensor: As the name suggests, the Sonar sensor also known as Ultrasonic sensor works based on an ultrasonic pulse time calculation to measure distance.


\begin{figure}[h]
  \centering
    \includegraphics[width=\linewidth]{figures/sonarSensor.png}
    \caption{A sonar sensor combined with microcontroller}
    \source {arrow.com}
\label{fig:sonarSensor}
\end{figure}


To measure the specific distance from the sensor, this can be calculated based on this formula \cite{sonar1}:

Distance = 1/2 T x C 


where T is time and C is the spped of sound in specific temparatuer of earth environment. At 20$^{\circ}$ C (68$^{\circ}$ F), the speed of sound is 343 meters/second (1125 feet/second), but this varies depending on temperature and humidity.


\subsubsection{IR Sensors}

These types of sensors sense the infrared light. In general, all objects on the infrared ( IR ) spectrum emit thermal radiation. That kind of radiation that is not visible to the human eye is measured by the infrared sensor. The biggest advantage of the IR sensor is that it uses very cheap and available and very easy to use. One of the major drawbacks of these sensors is that it pruned to background noise.

\begin{figure}[h]
  \centering
    \includegraphics[width=\linewidth]{figures/irSensor.jpg}
    \caption{An IR sensor architecture}
    \source {ram-e-shop.com}
\label{fig:irSensor}
\end{figure}


The working of IR sensor and sonar sensor is similar, where the sonar sensor emits soundwave and have a receiver for the sound wave the IR use infrared light instead of sound. 

\begin{figure}[h]
  \centering
    \includegraphics[width=0.5\linewidth]{figures/irSensorWork.jpg}
    \caption{Working principal of an IR sensor}
    \source {engineersgarage.com}
\label{fig:irSensorWork }
\end{figure}


\subsubsection{UV Sensors}

Such sensors measure the intensity or strength of the UV radiation incident. It has a longer wavelength than x-rays but is also shorter than the visible radiation. For robust ultraviolet sensing, an active material called polycrystalline diamond is used. UV sensors can detect exposure to ultraviolet radiation from the environment.


\begin{figure}[h]
  \centering
    \includegraphics[width=0.5\linewidth]{figures/uvSensor.jpg}
    \caption{UV sensor}
    \source {roboticsbd.com}
\label{fig:uvSensor }
\end{figure}


The UV sensor recognizes one energy signal type and transmits different energy signals. They are directed to an electric meter to observe and record these output signals. The output signals are transmitted for the generation of graphs and reports to a software-based computer through an analog-to-digital converter ( ADC).


\subsubsection{Touch Sensor}

A touch sensor functions as a variable resistor according to the position of the touch. As seen in figure \ref{touchSensorWork}

\begin{figure}[h]
  \centering
    \includegraphics[width=0.5\linewidth]{figures/touchSensorWork.png}
    \caption{Working mecanism of a touch sensor}
\label{fig:touchSensorWork }
\end{figure}

A touch sensor made of a substance that is absolutely conductive such as copper Isolated material for the spacing of foam or plastic or material which is partly conductive.
The way the touch sensor works is that the surface is partly conductive and opposes the present flow. The key concept of the linear position sensor is that when the length of this material, to be traveled by the current, is greater, the current flow is contrasted. The resistance of the material is therefore varied by adjusting the location at which it comprises the substance that is completely conductive.

\subsubsection{Global Positioning System (GPS) sensor}

We use the GPS sensor almost every day in our life, mostly via our phone or in our car, the GPS sensor is used to navigate around the world.

A minimum of 24 operational satellites orbit over 12,000 kilometers above ground at any given time. The locations of the satellites will always include up to 12 satellites in the sky above your location. The primary function of the 12 visible satellites is to relay radio frequency (1.1-1.5 GHz) information back to earth. A soil-based receiver or GPS module may measure its location and time with this information and other mathematical details. \cite{misra2006globa}



\subsubsection{LiDAR}

Lidar is an analogy for "light detection and ranging." The device uses eye-safe laser beams to produce a 3D view of the surroundings being scanned.


\begin{figure}[h]
  \centering
    \includegraphics[width=0.5\linewidth]{figures/lidarSensor.jpg}
    \caption{A Lidar device used for automotive}
    \source {Automotive Lidar Sensor Market 2017 - Leddar, Quanergy}
\label{fig:lidarSensor}
\end{figure}


A typical lidar sensor absorbs the ambient atmosphere with pulsed light waves. Such pulses rebound and come back to the sensor. To calculate the distances traveled, the sensor takes time for every pulse to return to the sensor. This procedure is repeated millions of times per second and creates a precise 3D environmental map in real-time. This chart can be used for secure surfing by an onboard computer.
Lidar is mostly used for advance autonomous systems to measure its surrounding environment.


\subsubsection{Gyroscope sensor}


A device that senses angular velocity is gyro sensors, also known as angular rate sensors or angular velocity sensors.  Gyroscopes in consumer électronics were not only found in compasses, ships, computer pointing instruments, etc.

\begin{figure}[h]
  \centering
    \includegraphics[width=0.5\linewidth]{figures/gyroSensor.png}
    \caption{An MMSE gyroscope}
    \source {controleverything.com}
\label{fig:gyroSensor}
\end{figure}

Since the gyroscope allows orientation and rotation to be measured, designers have combined them with modern technology. The gyroscope technology has made it possible to detect the movement inside a 3D world more reliably than the previous lone accelerometer on most smartphones. Gyroscopes are also paired with accelerometers for better direction and motion sensing in consumer electronics. The gyroscopes inside smartphones don't have wheels and gimbals such as the conventional mechanical ones in an old aircraft, they are MEMS (Micro-Electro-Mechanical Systems)  gyroscopes instead.



\subsubsection{Accelerometer sensor}



It can be used for the measurement of the acceleration exerted on the sensor, as its name implies. The acceleration is usually performed in two or three components of the axis-vector, which make up the acceleration. \cite{FERNANDEZ20138} There are a few uses of Accelerometers, remote controls for videos, smartphones, etc. Accelerometer usually combined with a Gyroscope for better sensing. Accelerometers give us two data types: Static force used on the sensor due to gravity detection in the direction of orientation and  Sensor strength, acceleration to motion, force detection

\begin{figure}[h]
  \centering
    \includegraphics[width=0.5\linewidth]{figures/accSensor.jpg}
    \caption{An electronic accelerometer sensor}
    \source {nuttyengineer.com}
\label{fig:accSensor}
\end{figure}



\subsubsection{Smart Sensors}

Due to the different needs, the sensor characteristics were adjusted a lot over time. The needs allowed intelligent and intelligent sensors to be developed. The IEEE 1451 standard defines smart sensors as a physical device or a combination of sensors with physical connectors for communication with processor and data network. \cite{5739775}.

Fig (smartSensorStructure) source cite [5739775]
\begin{figure}[h]
  \centering
    \includegraphics[width=\linewidth]{figures/smartSensorStructure.png}
    \caption{ Smart sensor building blocks }
    \source {Premier Farnell Ltd}
\label{fig:smartSensorStructure}
\end{figure}


The advent of smart sensors contributed to the development of microcontrollers used all over the world. Because of the intelligent sensors, interfacing and working with other peripherals as a single device was possible.
These smart sensors allow us to reduce data volume for communication with the main processor and made faster communication possible. It also reduced power consumption because multiple sensors are used together that use the same processing unit. The major benefit of these smart sensor is that they are typically small and very easy to integrate with other systems even with other sensors.

Here come the uses of IMU (Inertial Measurement Units). In the next section, we describe the properties of an IMU.


\subsection{IMU (Inertial Measurement Units)}
As our goal is to use IMU as our main sensor to collect data for acquiring objects position and orientation, this is our principal sensor to study. An IMU is typically a combination (Accelerometer, Gyroscope, Magnetometer) of sensors working together.
In this chapter, we will discuss the IMU, how its functions, its general structure, and the use cases.

\subsubsection{General structure}

The IMU typically consists of 3 individual sensors Accelerometer, Gyroscope, and a Magnetometer. Sometimes it comes with only an Accelerometer and Gyroscope. The Accelerometer, Gyroscope, and a Magnetometer provides the data of acceleration and velocity, orientation and angular velocity, gravitational forces respectively. This typically gives us the 9 axes of measurement on the other hand the IMU consist of only Accelerometer and Gyroscope provide 6 axes of measurement.
Early days the size of IMU was considerably larger compared to the current IMUs. Its because those are made mechanically but thanks to the MEMS or Micro-Electro-Mechanical Systems that are made possible to make these sensors electronically it's miniaturized the size considerably. \cite{MEMS}.
This helps us to integrate with other microcontrollers or microprocessors like Arduino or Raspberry Pi. Due to its miniature of size sometimes the IMUs are also known as MIMU (Miniature Inertial Measurement Units) \cite{2018233}

\begin{figure}[h]
  \centering
    \includegraphics[width=\linewidth]{figures/imuAG.png}
    \caption{ An IMU with 6DoF workiks with a magnetometer for 9DoF }
    \source {Stanford.edu lecture 9 of EE267: Virtual Reality S2020}
\label{fig:imuAG}
\end{figure}


\subsubsection{Working principal of IMU}
The main purpose of IMU is to measure and objects pose relative to its inertial space. Acceleration and angular speed are based on inertial principles are measured.

Accelerometers calculate in a particular direction linear acceleration. An accelerometer can be also used as a downward force to measure gravity. Integrating acceleration once shows a velocity estimate, and integrating again provides you with a position estimate.


\begin{equation}
f^{b}=R^{b n}\left(a_{i i}^{n}-g^{n}\right)
\end{equation}


Although accelerometers can measure linear acceleration, they are unable to measure orientation or rotational motion. Nevertheless, gyroscopes measure angular velocities along three axes: pitch (x-axis), roll (y-axis), and yaw (z-axis). While a gyroscope does not have an initial frame of reference (like gravity), for measuring angular position, you can combine its data with data from an accelerometer. Gyroscope data output angular velocity (deg/s).

The gyroscope measures the angular velocity of the body frame with respect to the inertial frame, expressed in the body frame \cite{ROS}, denoted by $\omega $. This angular velocity can be expressed as

\begin{equation}
\tilde{\omega}=\omega+b+\eta
\end{equation}

The estimated orientation can be obtained by integrating these angular velocities. We can also use Taylor series to measure discrete-time approximation of integration

%\begin{equation}{\theta}_{\text {gyro}}^{(t)}={\theta}_{\text {gyro}}^{(t-1)}+\tilde{{\omega}} \Delta t\end{equation}

Using 3 Gyro and Accelerometer we can obtain an estimated pose of the object in 3D space.
The working principle of a single unit of IMU can be seen in the figure \ref{fig:imuWP}


\begin{figure}[h]
  \centering
    \includegraphics[width=\linewidth]{figures/imuWP.png}
    \caption{ working principle of a single unit of IMU }
    \source {Stanford.edu lecture 9 of EE267: Virtual Reality S2020}
\label{fig:imuWP}
\end{figure}






\section{Sensor Fusion}
Sensor fusion is the act of combining data acquired from two or more sensors sources
such that the resulting combination of sensory information provides a more certain description of factors observed by the separate sensors than would be if used individ
ually [Elmenreich, 2001]. Sensor fusion is pertinent in many applications that entail
the use of multiple sensors for inference and control. Examples of applications in
clude intelligent and automated systems such as automotive driver assistance systems,
autonomous robotics, and manufacturing robotics [Elmenreich, 2007].
Sensor fusion methods aim to solve many of the problems inherently present in
sensors. Several important beneﬁts may be derived from sensor fusion systems over
single or disparate sensor sources. The beneﬁts of sensor fusion over single source are
the following [Elmenreich, 2001]:
\begin{itemize}
 \item Reliability: Using multiple sensor sources introduces more resilience to partial sensor failure, which leads to greater redundancy and reliability.
 \item Extended spatial coverage: Each sensor may cover diﬀerent areas. Combining the covered areas will lead to a greater overall coverage of surrounding
environment and accommodate sensor deprivation.
 \item Extended temporal coverage: Each sensor may update at diﬀerent time intervals, and thus interpolated sensor updates can be joined for increased tem
poral coverage and decreased sensor deprivation.
 \item Increased Conﬁdence: Combining sensor data will provide increased conﬁdence by providing measurements resilient to the uncertainties in any particular
sensor based on the combined coverage and error mitigation of all sensors. \item  Reduced Uncertainty: Given the resilience of multiple sensors to the speciﬁc uncertainty of any one, the overall uncertainty of the perception system can be
drastically reduced using sensor fusion.
 \item Robustness against noise: Multiple sensor sources can be used to determine when any one sensor has encountered noise in order to mitigate inﬂuence of
noise in the system.
 \item Increased Resolution: Multiple sensor sources can be used to increase the resolution of measurements by combining all observations.
\end{itemize}
According to [Rao, 2001], sensor fusion can yield results that outperform the measurements of the single best sensor in the system if the fusion function class satisﬁes a
proposed isolation property based on independently and identically distributed (iid)
samples. The fusion function classes outlined that satisfy this isolation property
are linear combinations, certain potential functions and feed-forward piecewise linear
neural networks based on minimization of empirical error per sample.



\section{Current Sensor Fusion Systems}
Many researchers and companies have developed their own versions of sensor fusion
systems for various purposes. Many of these systems are well-known an widely used
in practice within the ﬁelds of automotive and robotics. In [Steux et al., 2002], a
vehicle detection and tracking system using monocular color vision and radar data
fusion using a 3-layer belief network was proposed called FADE. The fusion system
focused on lower-level fusion and combined 12 diﬀerent features to generate target
position proposals each step and for each target. FADE performed in real-time and
yielded good detection results in most cases according to scenarios recorded in a real
car.
A fusion system for collision warning using a single camera and radar was applied
to detect and track vehicles in [Srinivasa et al., 2003]. The detections were fused using
a probabilistic framework in order to compute reliable vehicle depth and azimuth
angles. Their system clustered object detections into meta-tracks for each object and
fused object tracks between the sensors. They found that the radar had many false
positives due to multiple detections on large vehicles, structures, roadway signs and overhead structures. They also found that the camera had false positive detections on
larger vehicles and roadway noise like potholes. Their system worked appropriately
for nearby vehicles that were clearly visible by both sensors, but the system failed to
detect vehicles more than 100 meters away due to insuﬃcient resolution or vehicle
occlusion.
In [Dagan et al., 2004], engineers from Mobileye successfully applied a camera
system to compute the time to collision (TTC) course from size and position of
vehicles in the image. Although they did not test this theory, they mentioned the
future use of radar and camera in a sensor fusion system since the radar would
give more accurate range and range-rate measurements while the vision would solve
angular accuracy problems of the radar. When the research was conducted, it was
suggested that the fusion solution between radar and camera was costly, but since
then, costs have decreased.
A collision mitigation fusion system using a laser-scanner and stereo-vision was
constructed and tested in [Labayrade et al., 2005]. The combination of the compli
mentary laser scanner and stereo-vision sensors provided a high detection rate, low
false alarm rate, and a system reactive to many obstacle occurrences. They men
tioned that the laser-scanner was fast and accurate but could not be used alone due
to many false alarms from collisions with the road surface and false detections with
laser passes over obstacles. They also mentioned that stereo-vision was useful for
modeling road geometry and obstacle detection, but it was not accurate for comput
ing precise velocities or TTC for collision mitigation.
In [Laneurit et al., 2003], a Kalman ﬁlter was successfully developed and applied
for the purpose of sensor fusion between multiple sensors including GPS, wheel angle
sensor, camera and LiDAR. They showed that this system was useful for detection
and localization of vehicles on the road, especially when using the wheel angle sensor
for detecting changes in vehicle direction. Their results revealed that cooperation between the positioning sensors for obstacle detection and location paired with LiDAR
were able to improve global positioning of vehicles.
A deep learning framework for signal estimation and classiﬁcation applicable for
mobile devices was created and tested in [Yao et al., 2016]. This framework applied
convolutional and recurrent layers for regression and classiﬁcation mobile comput
ing tasks. The framework exploited local interactions of diﬀerent sensing modalities
using convolutional neural network (CNN)s, merged them into a global interaction
and extracted temporal relationships via stacked GRU or LSTM layers. Their frame
work achieved a notable mean absolute error on vehicle tracking regression tasks as
compared to existing sensor fusion systems and high accuracy on human activity
recognition classiﬁcation tasks while it remained eﬃcient enough to use on mobile
devices like the Google Nexus 5 and Intel Edison.
A multimodal, multi-stream deep learning framework designed to tackle the ego
centric activity recognition using data fusion was proposed in [Song et al., 2016b]. To
begin, they extended a multi-stream CNN to learn spatial and temporal features from
egocentric videos. Then, they proposed a multi-stream LSTM architecture to learn
features from multiple sensor streams including accelerometer and gyroscope. Third,
they proposed a two-level fusion technique using SoftMax classiﬁcation layers and dif
ferent pooling methods to fuse the results of the neural networks in order to classify
egocentric activities. The system performed worse than a hand-crafted multi-modal
Fisher vector, but it was noted that hand-crafted features tended to perform better
on smaller datasets. In review of the research, it seems there were limited amounts of
data, ﬂaws in the fusion design with SoftMax combination and ﬂaws in the sensors,
such as limited sensing capabilities. These factors all may have led to worse results
than hand-crafted features on the utilized dataset.
In [Wu et al., 2015], a multi-stream deep fusion neural network system using con
volution neural networks and LSTM layers was applied to classify multi-modal temporal stream information in videos. Their adaptive multi-stream fusion system achieved
an accuracy level much higher than other methods of fusion including averaging, ker
nel averaging, multiple kernel learning (MKL), and logistic regression fusion methods.



\section{Machine Learning}
Machine Learning is a term that describes algorithms that learn from data.
Goodfellow et al. quotes Mitchell from 1996 for a definition of
what learning means, A computer program is said to learn from experience
E with respect to some class of tasks T and performance measure P, if its
performance at tasks in T, as measured by P, improves with experience E.
There is a wide variety of tasks T that can be done. For instance common
tasks are regression, classification, anomaly detection, denoising and translation.
Depending on what experience E they see during the learning process the most
algorithms can be classified as supervised or unsupervised learning. Supervised
learning algorithms experience a data set together with target values or labels
that act as instructions on what to do. The unsupervised learning algorithms
don’t have any labels or targets but instead try to learn about the structure of
the data set. The performance P can be how far off from the labels the output
of the algorithm is in the case of supervised learning. This performance measure
is often difficult to choose.



\section{Deep Learning}
Deep Learning is a term used for training deep artificial neural networks, or
ANN s. An ANN is a function approximator consisting of multiple functions,

also called neurons. An ANN is called a feedforward ANN if there are no
feedback connections of the output of the ANN being sent to itself. Each neuron
is a linear function of some inputs, fed into a nonlinear function, called activation
function. If the input of a neuron is a vector x, the output of the neuron gi can
be written as


\subsection{Convulitional Neural Network}
The non-linear functions called activation functions, such as K in Equation 2.1,
are typically fixed non-linear functions that are used to make the ANN able to
approximate non-linear functions. If the non-linear activation functions weren’t
used, the output of the ANN would still be a linear function of the inputs x. In deep learning, there are a few commonly used functions that
have become standard as activation functions.
The rectifying linear unit, or ReLU is a function that is defined as


\subsection{Back-propagation}
When training a ANN to approximate a function, gradient based optimization
is commonly used. To compute the gradient of a (loss) function f with
respect to the parameters  an algorithm called back-propagation is commonly
used. It back-propagates from the objective function to gradients of the different
weights and biases in the ANN to compute the gradient of the objective function with respect to all the parameters. If we have a function 
can be a cost function in a supervised learning setting but could also be other
functions like a reward function in the reinforcement learning setting that will

\subsection{Gradient based optimization for deep learning}

The ANN weights and biases are updated to optimize some objective function
with gradient based optimization, using the gradients computed with the backprop algorithm.
The classic algorithm for optimizing the parameters in a ANN is called Gradient
Descent. be an objective function that we want to minimize, where
x is the input to the ANN the trainable parameters of the ANN. Then
moving in the opposite direction of the gradient of L with respect to  we move
the parameters in a direction that makes the objective function smaller, which is
what we want. However, usually x are random samples and the true gradient is
the expected value of the gradient with respect to the random samples actually
used. Thus when computing the gradient of the loss function as a function of
some samples x, we are computing an unbiased noisy estimate of the gradient.
This is referred to as Stochastic Gradient Descent, or SGD. Stochastic Gradient

\subsection{Batch Normalization}
Batch Normalization is a recent method in deep learning used to be able to train
networks faster and use higher learning rates with decreased risk of divergence.
It does so by making the normalization a part of the model architecture, fixing
the mean and variances of the inputs to a layer by a normalization step. This
makes the risk of the inputs to the activation functions getting in a range where
the gradient vanishes smaller and allows for the use of higher learning rates. 

\section{Deep Learning}
Deep Learning is a term used for training deep artificial neural networks, or
ANN s. An ANN is a function approximator consisting of multiple functions,

also called neurons. An ANN is called a feedforward ANN if there are no
feedback connections of the output of the ANN being sent to itself. Each neuron
is a linear function of some inputs, fed into a nonlinear function, called activation
function. If the input of a neuron is a vector x, the output of the neuron gi can
be written as


\subsection{Activation Functions}
The non-linear functions called activation functions, such as K in Equation 2.1,
are typically fixed non-linear functions that are used to make the ANN able to
approximate non-linear functions. If the non-linear activation functions weren’t
used, the output of the ANN would still be a linear function of the inputs x. In deep learning, there are a few commonly used functions that
have become standard as activation functions.
The rectifying linear unit, or ReLU is a function that is defined as

\subsection{Cost Functions}
To train a ANN, cost functions are usually minimized. In supervised learning, where we have labeled data to learn from, the cost functions are more
straightforward than for instance in reinforcement learning that we will discuss
in section 
The two main problems of supervised learning are regression and classification.
In regression one wants to predict a numerical value whereas in classification the
goal is to predict which class something belongs to given some inputs. Different
loss functions are common for regression and classification. Most of them are
however derived from the same principle, the one of Maximum Likelihood.

\subsection{Back-propagation}
When training a ANN to approximate a function, gradient based optimization
is commonly used. To compute the gradient of a (loss) function f with
respect to the parameters  an algorithm called back-propagation is commonly
used. It back-propagates from the objective function to gradients of the different
weights and biases in the ANN to compute the gradient of the objective function with respect to all the parameters. If we have a function 
can be a cost function in a supervised learning setting but could also be other
functions like a reward function in the reinforcement learning setting that will

\subsection{Gradient based optimization for deep learning}

The ANN weights and biases are updated to optimize some objective function
with gradient based optimization, using the gradients computed with the backprop algorithm.
The classic algorithm for optimizing the parameters in a ANN is called Gradient
Descent. be an objective function that we want to minimize, where
x is the input to the ANN the trainable parameters of the ANN. Then
moving in the opposite direction of the gradient of L with respect to  we move
the parameters in a direction that makes the objective function smaller, which is
what we want. However, usually x are random samples and the true gradient is
the expected value of the gradient with respect to the random samples actually
used. Thus when computing the gradient of the loss function as a function of
some samples x, we are computing an unbiased noisy estimate of the gradient.
This is referred to as Stochastic Gradient Descent, or SGD. Stochastic Gradient


